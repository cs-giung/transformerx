# Perplexity

The example demonstrates computing perplexity.
```bash
python examples/perplexity/run.py --model meta-llama/Meta-Llama-3-8B
```

## WikiText-2

| `model`                               | `FP16`| `Q8_0`| `Q7_0`| `Q6_0`| `Q5_0`| `Q4_0`| `Q3_0`|
| :-                                    | :-:   | :-:   | :-:   | :-:   | :-:   | :-:   | :-:   |
| `huggyllama/llama-7b`                 | 
| `meta-llama/Llama-2-7b-chat-hf`       | 
| `meta-llama/Llama-2-7b-hf`            | 
| `meta-llama/Meta-Llama-3-8B-Instruct` | 
| `meta-llama/Meta-Llama-3-8B`          | 
| `microsoft/Phi-3-medium-4k-instruct`  | 
| `microsoft/Phi-3-mini-4k-instruct`    | 
| `mistral-community/Mistral-7B-v0.2`   | 
| `mistralai/Mistral-7B-Instruct-v0.1`  | 
| `mistralai/Mistral-7B-Instruct-v0.2`  | 
| `mistralai/Mistral-7B-Instruct-v0.3`  | 
| `mistralai/Mistral-7B-v0.1`           | 
| `mistralai/Mistral-7B-v0.3`           | 

## PTB

| `model`                               | `FP16`| `Q8_0`| `Q7_0`| `Q6_0`| `Q5_0`| `Q4_0`| `Q3_0`|
| :-                                    | :-:   | :-:   | :-:   | :-:   | :-:   | :-:   | :-:   |
| `huggyllama/llama-7b`                 | 
| `meta-llama/Llama-2-7b-chat-hf`       | 
| `meta-llama/Llama-2-7b-hf`            | 
| `meta-llama/Meta-Llama-3-8B-Instruct` | 
| `meta-llama/Meta-Llama-3-8B`          | 
| `microsoft/Phi-3-medium-4k-instruct`  | 
| `microsoft/Phi-3-mini-4k-instruct`    | 
| `mistral-community/Mistral-7B-v0.2`   | 
| `mistralai/Mistral-7B-Instruct-v0.1`  | 
| `mistralai/Mistral-7B-Instruct-v0.2`  | 
| `mistralai/Mistral-7B-Instruct-v0.3`  | 
| `mistralai/Mistral-7B-v0.1`           | 
| `mistralai/Mistral-7B-v0.3`           | 
